
#!/usr/bin/env python3
# -*- coding: utf-8 -*-


from __future__ import annotations

import os
from pathlib import Path
from datetime import datetime, timedelta
import argparse
import sys
import re

import pandas as pd


# ========= USER PATHS (adjust if needed) =========
# History folder containing daily CSVs:
HIST_FILES_DIR = Path(r"//Msad/root/NA/NY/LIB/IT/EoL_Program/Reporting_Framework/ServiceNow Discoveries/VMARC/outputs_hist")
# =================================================


# ---------- Utilities ----------

def debug(msg: str) -> None:
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}", flush=True)


def list_hist_files(folder: Path) -> list[Path]:
    if not folder.exists():
        raise FileNotFoundError(f"History folder not found: {folder}")
    return sorted([p for p in folder.iterdir() if p.is_file() and p.suffix.lower() == ".csv"],
                  key=lambda p: p.stat().st_mtime, reverse=True)


def grab_last_file(folder: Path) -> tuple[Path | None, Path | None]:
    files = list_hist_files(folder)
    if not files:
        return None, None
    latest = files[0]
    previous = files[1] if len(files) > 1 else None
    return latest, previous


def safe_read_csv(path: Path, usecols: list[str] | None = None, dtypes: dict | None = None) -> pd.DataFrame:
    """
    Read CSV with sensible defaults; falls back if pyarrow engine is unavailable.
    """
    kwargs = dict(low_memory=False)
    if usecols is not None:
        kwargs["usecols"] = usecols
    if dtypes is not None:
        kwargs["dtype"] = dtypes

    try:
        df = pd.read_csv(path, engine="pyarrow", **kwargs)  # fastest & consistent dtypes
    except Exception:
        df = pd.read_csv(path, **kwargs)
    return df


def _pick_column(df: pd.DataFrame, *candidates: str) -> str | None:
    """
    Find first header present among case/whitespace variations.
    Returns actual column name in df or None.
    """
    normalized = {c.lower().strip(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower().strip()
        if key in normalized:
            return normalized[key]
    return None


def canonize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Return a copy with canonical columns:
      md_value, md_data_source, Status
    Accepts common variants seen in your screenshots.
    """
    df = df.copy()
    df.columns = [c.strip() for c in df.columns]

    mv = _pick_column(df, "md_value", "md_value_Ia1", "md_value_1a1", "md_value_ta1")
    mds = _pick_column(df, "md_data_source", "md_data_source_Ia1", "md_data_source_1a1", "md_data_source_ta1")
    status = _pick_column(df, "Status", "status")

    missing = []
    if mv is None:
        missing.append("md_value")
    if mds is None:
        missing.append("md_data_source")
    if status is None:
        missing.append("Status")

    if missing:
        raise KeyError(f"Missing expected columns: {missing}. Found: {list(df.columns)}")

    out = df[[mv, mds, status]].rename(columns={mv: "md_value",
                                                mds: "md_data_source",
                                                status: "Status"})
    return out


def add_joining_keys(df: pd.DataFrame) -> pd.DataFrame:
    """
    Build normalized join keys. If keys already exist, keep them; else create.
    """
    df = df.copy()
    if "md_value_key" not in df.columns:
        df["md_value_key"] = df["md_value"]
    if "md_data_source_key" not in df.columns:
        df["md_data_source_key"] = df["md_data_source"]
    return df


def discard_status_previous(df: pd.DataFrame) -> pd.DataFrame:
    if "Status_previous_day" in df.columns:
        return df.drop(columns=["Status_previous_day"])
    return df


def dedupe_previous(prev_df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensure exactly one row per (md_value_key, md_data_source_key).
    If duplicates exist, keep the last occurrence (file order or prior sort).
    """
    key_cols = ["md_value_key", "md_data_source_key"]
    # Keep last occurrence per key
    deduped = prev_df.drop_duplicates(subset=key_cols, keep="last").copy()
    return deduped


def estimate_worst_case_rows(left: pd.DataFrame, right: pd.DataFrame) -> int:
    key = ["md_value_key", "md_data_source_key"]
    a_mult = left.groupby(key).size()
    b_mult = right.groupby(key).size()
    worst = int((a_mult * b_mult.reindex(a_mult.index, fill_value=0)).sum())
    return worst


def make_output_name(folder: Path, ref_date: datetime) -> Path:
    return folder / f"SN_resources_query_status_{ref_date.strftime('%Y-%m-%d')}.csv"


# ---------- Main ----------

def main() -> int:
    parser = argparse.ArgumentParser(description="Process input reference date and compare latest vs previous CSV.")
    parser.add_argument("--reference_date",
                        type=str,
                        default=datetime.today().strftime("%Y-%m-%d"),
                        help="Reference date in YYYY-MM-DD (defaults to today).")
    args = parser.parse_args()

    # Parse the reference date
    try:
        reference_date = datetime.strptime(args.reference_date, "%Y-%m-%d")
    except ValueError:
        print("Error: reference_date should be YYYY-MM-DD", file=sys.stderr)
        return 2

    debug(f"Reference date timestamp: {reference_date.isoformat()}")

    # The script always compares FILES by modified time in the history folder
    latest_file, previous_file = grab_last_file(HIST_FILES_DIR)

    if latest_file is None:
        print(f"No CSV files found in {HIST_FILES_DIR}", file=sys.stderr)
        return 1

    debug(f"Latest file   : {latest_file.name}")
    if previous_file:
        debug(f"Previous file : {previous_file.name}")
    else:
        debug("Previous file : <none> (first run)")

    # Read files, normalize schema
    # (Only columns we need; dtypes as string to avoid object bloat)
    usecols = None  # read all, then canonize trims to needed
    dtypes = None

    today_df_raw = safe_read_csv(latest_file, usecols=usecols, dtypes=dtypes)
    today_df = canonize_columns(today_df_raw)
    today_df = add_joining_keys(discard_status_previous(today_df))

    if previous_file is not None:
        prev_df_raw = safe_read_csv(previous_file, usecols=usecols, dtypes=dtypes)
        prev_df = canonize_columns(prev_df_raw)
        prev_df = add_joining_keys(prev_df)

        # Deduplicate to 1 row per key (critical to avoid m:m)
        prev_1row = dedupe_previous(prev_df).rename(columns={"Status": "Status_previous_day"})

        # Sanity diagnostics
        key_cols = ["md_value_key", "md_data_source_key"]
        prev_dups = prev_df.duplicated(key_cols, keep=False).sum()
        debug(f"Previous file duplicate-key rows: {int(prev_dups)}")

        worst = estimate_worst_case_rows(today_df, prev_1row)
        debug(f"Estimated worst-case merged rows: {worst:,}")
        if worst > 50_000_000:
            raise MemoryError(f"Merge would create ~{worst:,} rows. Aborting to protect memory.")

        # Many-to-one merge with validation and indicator
        merged = today_df.merge(
            prev_1row,
            on=key_cols,
            how="left",
            copy=False,
            validate="m:1",
            indicator=True
        )
        debug("Merge indicator counts:\n" + str(merged["_merge"].value_counts()))
        merged.drop(columns=["_merge"], inplace=True)

        # Keep original + new column
        original_cols = [c for c in today_df.columns if c not in ("Status_previous_day",)]
        out_df = merged[original_cols + ["Status_previous_day"]]
    else:
        # No previous file â†’ just add empty Status_previous_day
        out_df = today_df.copy()
        out_df["Status_previous_day"] = pd.NA

    # Decide output name based on provided reference date (matches your pattern)
    out_path = make_output_name(HIST_FILES_DIR, reference_date)
    debug(f"Saving file for dashboard: {out_path.name}")
    out_df.to_csv(out_path, index=False)

    # Also keep a same-day "absolute" copy if your flow expects it (optional).
    # Comment this out if you only want the dated filename above.
    # absolute_copy = HIST_FILES_DIR / out_path.name
    # if absolute_copy != out_path:
    #     out_df.to_csv(absolute_copy, index=False)

    debug("Done.")
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except pd.errors.MergeError as e:
        # validation='m:1' or schema mismatches will land here with a clear message
        print(f"MergeError: {e}", file=sys.stderr)
        sys.exit(3)
    except MemoryError as e:
        print(f"MemoryError: {e}", file=sys.stderr)
        sys.exit(4)
    except KeyError as e:
        print(f"KeyError (schema drift): {e}", file=sys.stderr)
        sys.exit(5)
    except Exception as e:
        print(f"Unexpected error: {type(e).__name__}: {e}", file=sys.stderr)
        sys.exit(6)
